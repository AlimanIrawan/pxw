#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Flask Webåº”ç”¨ - æ–°é—»çˆ¬è™«æ§åˆ¶ç•Œé¢
ç”¨äºäº‘ç«¯éƒ¨ç½²çš„Webæ§åˆ¶å°
"""

import os
import json
from datetime import datetime, timedelta
from flask import Flask, render_template, request, jsonify, send_file
from threading import Thread
import logging
import schedule
import time

from detik_crawler import DetikCrawler
from simple_crawler import SimpleCrawler
from data_processor import DataProcessor
from config import ConfigManager
from logger import get_logger

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'your-secret-key-change-in-production')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = get_logger()

def add_task_log(message, level='info'):
    """æ·»åŠ æ—¥å¿—åˆ°ä»»åŠ¡çŠ¶æ€"""
    from datetime import datetime
    timestamp = datetime.now().strftime('%H:%M:%S')
    log_entry = {
        'timestamp': timestamp,
        'level': level,
        'message': message
    }
    task_status['logs'].append(log_entry)
    
    # åªä¿ç•™æœ€è¿‘100æ¡æ—¥å¿—
    if len(task_status['logs']) > 100:
        task_status['logs'] = task_status['logs'][-100:]
    
    # åŒæ—¶å†™å…¥æ ‡å‡†æ—¥å¿—
    if level == 'error':
        logger.error(message)
    elif level == 'warning':
        logger.warning(message)
    else:
        logger.info(message)

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡çŠ¶æ€
task_status = {
    'running': False,
    'progress': 0,
    'message': 'å‡†å¤‡å°±ç»ª',
    'total_news': 0,
    'current_news': 0,
    'output_files': [],
    'logs': []  # å®æ—¶æ—¥å¿—ç¼“å­˜
}

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template('index.html')

@app.route('/start_crawl', methods=['POST'])
def start_crawl():
    """å¼€å§‹çˆ¬å–ä»»åŠ¡"""
    global task_status
    
    if task_status['running']:
        return jsonify({'error': 'çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ'})
    
    data = request.get_json()
    target_date = data.get('date')
    
    if not target_date:
        return jsonify({'error': 'è¯·é€‰æ‹©æ—¥æœŸ'})
    
    # é‡ç½®ä»»åŠ¡çŠ¶æ€
    task_status = {
        'running': True,
        'progress': 0,
        'message': 'æ­£åœ¨åˆå§‹åŒ–...',
        'total_news': 0,
        'current_news': 0,
        'output_files': [],
        'logs': []
    }
    
    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«
    thread = Thread(target=run_crawler, args=(target_date,))
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'çˆ¬è™«å·²å¯åŠ¨'})

@app.route('/status')
def get_status():
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    return jsonify(task_status)

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½æ–‡ä»¶"""
    file_path = os.path.join('output', filename)
    if os.path.exists(file_path):
        return send_file(file_path, as_attachment=True)
    else:
        return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})

@app.route('/logs')
def get_logs():
    """è·å–æœ€æ–°æ—¥å¿—"""
    try:
        log_files = [f for f in os.listdir('logs') if f.endswith('.log')]
        if log_files:
            latest_log = max(log_files)
            log_path = os.path.join('logs', latest_log)
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # è¿”å›æœ€å50è¡Œæ—¥å¿—
                return jsonify({'logs': lines[-50:]})
    except Exception as e:
        logger.error(f"è·å–æ—¥å¿—å¤±è´¥: {e}")
    
    return jsonify({'logs': []})

def run_crawler(target_date):
    """è¿è¡Œçˆ¬è™«çš„åå°ä»»åŠ¡"""
    global task_status
    
    try:
        # æ­¥éª¤1: åˆå§‹åŒ–
        task_status['message'] = 'ğŸ”§ æ­£åœ¨åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...'
        task_status['progress'] = 5
        task_status['logs'] = []  # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—
        add_task_log(f"ğŸš€ å¼€å§‹çˆ¬å–ä»»åŠ¡ï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        config = ConfigManager()
        task_status['progress'] = 10
        task_status['message'] = 'ğŸ”§ æ­£åœ¨è®¾ç½®çˆ¬è™«é…ç½®...'
        add_task_log("ğŸ“ åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨")
        
        processor = DataProcessor(config)
        task_status['progress'] = 15
        add_task_log("ğŸ’¾ åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨")
        
        # å°è¯•ä½¿ç”¨Chromeçˆ¬è™«ï¼Œå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–çˆ¬è™«
        crawler = None
        use_simple_crawler = False
        
        if os.environ.get('RENDER'):
            # äº‘ç«¯ç¯å¢ƒï¼šç›´æ¥ä½¿ç”¨ç®€åŒ–çˆ¬è™«
            task_status['message'] = 'ğŸ”§ äº‘ç«¯ç¯å¢ƒï¼Œä½¿ç”¨ç®€åŒ–çˆ¬è™«...'
            add_task_log("â˜ï¸ æ£€æµ‹åˆ°äº‘ç«¯ç¯å¢ƒï¼Œä½¿ç”¨ç®€åŒ–çˆ¬è™«")
            crawler = SimpleCrawler(config)
            use_simple_crawler = True
            task_status['progress'] = 20
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šå°è¯•Chromeçˆ¬è™«
            try:
                task_status['message'] = 'ğŸ•·ï¸ æ­£åœ¨å¯åŠ¨Chromeæµè§ˆå™¨...'
                add_task_log("ğŸŒ æœ¬åœ°ç¯å¢ƒï¼Œå°è¯•å¯åŠ¨Chromeçˆ¬è™«")
                crawler = DetikCrawler(config)
                task_status['progress'] = 20
            except Exception as e:
                add_task_log(f"âš ï¸ Chromeçˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}", "warning")
                task_status['message'] = 'ğŸ”§ Chromeå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çˆ¬è™«...'
                add_task_log("ğŸ”„ åˆ‡æ¢åˆ°ç®€åŒ–çˆ¬è™«")
                crawler = SimpleCrawler(config)
                use_simple_crawler = True
                task_status['progress'] = 20
        
        # è‡ªå®šä¹‰è¿›åº¦å›è°ƒ
        def progress_callback(current, total, message):
            if total > 0:
                crawl_progress = 20 + (current / total) * 50  # 20-70%çš„è¿›åº¦ç”¨äºçˆ¬å–
                task_status['progress'] = int(crawl_progress)
                task_status['current_news'] = current
                task_status['total_news'] = total
                task_status['message'] = message
        
        # çˆ¬å–æ–°é—»
        add_task_log(f"ğŸš€ å¼€å§‹çˆ¬å–æ–°é—»æ•°æ®")
        news_data = crawler.crawl_news(target_date)
        
        if not news_data:
            task_status['running'] = False
            task_status['message'] = 'âŒ æœªè·å–åˆ°æ–°é—»æ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥æœŸæˆ–ç½‘ç»œè¿æ¥'
            task_status['progress'] = 0
            add_task_log("âŒ çˆ¬å–ç»“æœä¸ºç©º", "error")
            return
        
        task_status['total_news'] = len(news_data)
        task_status['current_news'] = len(news_data)
        task_status['progress'] = 70
        task_status['message'] = f'âœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(news_data)} ç¯‡æ–°é—»'
        add_task_log(f"âœ… çˆ¬å–æˆåŠŸï¼Œå…±è·å– {len(news_data)} ç¯‡æ–°é—»", "success")
        
        # æ­¥éª¤3: ä¿å­˜æ•°æ®
        task_status['message'] = 'ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®æ–‡ä»¶...'
        task_status['progress'] = 75
        add_task_log("ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶")
        
        try:
            output_file = processor.save_news_data(news_data, target_date)
            task_status['progress'] = 80
            task_status['message'] = 'âœ… æ•°æ®æ–‡ä»¶ä¿å­˜å®Œæˆ'
            add_task_log(f"âœ… æ•°æ®æ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_file}", "success")
        except Exception as e:
            add_task_log(f"âŒ æ•°æ®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}", "error")
            task_status['running'] = False
            task_status['message'] = 'âŒ æ•°æ®æ–‡ä»¶ä¿å­˜å¤±è´¥'
            return
        
        # è®°å½•è¾“å‡ºæ–‡ä»¶
        output_dir = config.get_output_dir()
        files = []
        for filename in os.listdir(output_dir):
            if target_date in filename and filename.endswith('.txt'):
                file_path = os.path.join(output_dir, filename)
                if os.path.exists(file_path):
                    files.append({
                        'name': filename,
                        'size': os.path.getsize(file_path),
                        'url': f'/download/{filename}'
                    })
        
        task_status['output_files'] = files
        task_status['progress'] = 85
        logger.info(f"ç”Ÿæˆæ–‡ä»¶: {[f['name'] for f in files]}")
        
        # æ­¥éª¤4: æäº¤åˆ°GitHubï¼ˆå¦‚æœåœ¨äº‘ç«¯ç¯å¢ƒï¼‰
        if os.environ.get('RENDER'):
            task_status['message'] = 'ğŸ“¤ æ­£åœ¨ä¸Šä¼ åˆ°GitHub...'
            task_status['progress'] = 90
            commit_to_github(target_date, files)
            task_status['message'] = 'âœ… GitHubä¸Šä¼ å®Œæˆ'
            task_status['progress'] = 95
        else:
            task_status['progress'] = 95
        
        # å®Œæˆ
        task_status['progress'] = 100
        task_status['message'] = f'ğŸ‰ ä»»åŠ¡å®Œæˆï¼å…±çˆ¬å– {len(news_data)} ç¯‡æ–°é—»ï¼Œç”Ÿæˆ {len(files)} ä¸ªæ–‡ä»¶'
        logger.info(f"çˆ¬å–ä»»åŠ¡å®Œæˆ: {len(news_data)}ç¯‡æ–°é—», {len(files)}ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"çˆ¬è™«ä»»åŠ¡å¤±è´¥: {error_msg}", exc_info=True)
        task_status['message'] = f'âŒ ä»»åŠ¡å¤±è´¥: {error_msg}'
        task_status['progress'] = 0
        
        # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å…·ä½“çš„æç¤º
        if 'ChromeDriver' in error_msg or 'Chrome' in error_msg:
            task_status['message'] += ' (Chromeæµè§ˆå™¨é—®é¢˜)'
        elif 'timeout' in error_msg.lower():
            task_status['message'] += ' (ç½‘ç»œè¶…æ—¶)'
        elif 'connection' in error_msg.lower():
            task_status['message'] += ' (ç½‘ç»œè¿æ¥é—®é¢˜)'
            
    finally:
        task_status['running'] = False

def commit_to_github(target_date, files):
    """æäº¤æ–‡ä»¶åˆ°GitHub"""
    try:
        import subprocess
        import shutil
        
        # åˆ›å»ºæ—¥æœŸç›®å½•
        date_dir = os.path.join('output', target_date)
        os.makedirs(date_dir, exist_ok=True)
        
        # ç§»åŠ¨æ–‡ä»¶åˆ°æ—¥æœŸç›®å½•
        for file_info in files:
            filename = file_info['name']
            src = os.path.join('output', filename)
            dst = os.path.join(date_dir, filename)
            if os.path.exists(src):
                shutil.move(src, dst)
        
        # å¤åˆ¶åˆ°latestç›®å½•
        latest_dir = os.path.join('output', 'latest')
        os.makedirs(latest_dir, exist_ok=True)
        
        for file_info in files:
            filename = file_info['name']
            src = os.path.join(date_dir, filename)
            # é‡å‘½åä¸ºlatest
            dst_name = filename.replace(target_date, 'latest')
            dst = os.path.join(latest_dir, dst_name)
            if os.path.exists(src):
                shutil.copy2(src, dst)
        
        # Gitæ“ä½œ
        subprocess.run(['git', 'add', 'output/'], check=True)
        commit_msg = f"Auto crawl: {target_date} - {len(files)} files"
        subprocess.run(['git', 'commit', '-m', commit_msg], check=True)
        subprocess.run(['git', 'push'], check=True)
        
        logger.info(f"æˆåŠŸæäº¤åˆ°GitHub: {commit_msg}")
        
    except Exception as e:
        logger.error(f"GitHubæäº¤å¤±è´¥: {e}")

def daily_auto_crawl():
    """æ¯æ—¥è‡ªåŠ¨çˆ¬å–ä»»åŠ¡"""
    try:
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        logger.info(f"å¼€å§‹æ‰§è¡Œæ¯æ—¥è‡ªåŠ¨çˆ¬å–ä»»åŠ¡ï¼Œç›®æ ‡æ—¥æœŸ: {yesterday}")
        
        config = ConfigManager()
        crawler = DetikCrawler(config)
        processor = DataProcessor(config)
        
        # çˆ¬å–æ–°é—»
        news_data = crawler.crawl_news(yesterday)
        
        if news_data:
            # ä¿å­˜æ•°æ®
            processor.save_news_data(news_data, yesterday)
            
            # æäº¤åˆ°GitHubï¼ˆå¦‚æœåœ¨äº‘ç«¯ç¯å¢ƒï¼‰
            if os.environ.get('RENDER'):
                from daily_task import organize_and_commit_files
                organize_and_commit_files(yesterday, logger)
            
            logger.info(f"æ¯æ—¥è‡ªåŠ¨çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(news_data)} ç¯‡æ–°é—»")
        else:
            logger.warning("æ¯æ—¥è‡ªåŠ¨çˆ¬å–æœªè·å–åˆ°æ•°æ®")
            
    except Exception as e:
        logger.error(f"æ¯æ—¥è‡ªåŠ¨çˆ¬å–å¤±è´¥: {e}")

def setup_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
    # æ¯å¤©å‡Œæ™¨3ç‚¹UTCæ‰§è¡Œï¼ˆåŒ—äº¬æ—¶é—´11ç‚¹ï¼‰
    schedule.every().day.at("03:00").do(daily_auto_crawl)
    logger.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼šæ¯å¤©å‡Œæ™¨3ç‚¹UTCè‡ªåŠ¨çˆ¬å–")
    
    def run_scheduler():
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œå®šæ—¶å™¨
    scheduler_thread = Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()

def ensure_directories():
    """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    directories = ['output', 'logs', 'templates', 'output/latest']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        if logger:
            logger.info(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {directory}")

# åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºç›®å½•
ensure_directories()

if __name__ == '__main__':
    # åº”ç”¨å·²ç»åˆ›å»ºäº†ç›®å½•
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆä»…åœ¨äº‘ç«¯ç¯å¢ƒï¼‰
    if os.environ.get('RENDER'):
        setup_scheduler()
        logger.info("äº‘ç«¯ç¯å¢ƒï¼šå®šæ—¶ä»»åŠ¡å·²å¯åŠ¨")
    else:
        logger.info("æœ¬åœ°ç¯å¢ƒï¼šè·³è¿‡å®šæ—¶ä»»åŠ¡è®¾ç½®")
    
    # è¿è¡Œåº”ç”¨
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
