#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Detikç½‘ç«™çˆ¬è™«æ¨¡å—
è´Ÿè´£ä»detik.comçˆ¬å–æ–°é—»æ•°æ®
"""

import time
import os
import requests
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import pytz
import re
from logger import get_logger

class DetikCrawler:
    """Detikç½‘ç«™çˆ¬è™«"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.config = config
        self.logger = get_logger()
        self.base_url = config.get_detik_base_url()
        self.request_delay = config.get_request_delay()
        self.max_retries = config.get_max_retries()
        self.request_timeout = config.get_request_timeout()
        
        # è®¾ç½®è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def _is_cloud_environment(self) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨äº‘ç«¯ç¯å¢ƒä¸­"""
        cloud_indicators = [
            'RENDER',           # Render
            'HEROKU',           # Heroku  
            'RAILWAY',          # Railway
            'VERCEL',           # Vercel
            'NETLIFY',          # Netlify
            'GOOGLE_CLOUD_PROJECT',  # Google Cloud
            'AWS_LAMBDA_FUNCTION_NAME',  # AWS Lambda
        ]
        return any(os.environ.get(key) for key in cloud_indicators)
    
    def _setup_driver(self) -> webdriver.Chrome:
        """è®¾ç½®Chrome WebDriver
        
        Returns:
            é…ç½®å¥½çš„Chrome WebDriverå®ä¾‹
        """
        chrome_options = Options()
        
        # æ£€æµ‹ç¯å¢ƒç±»å‹
        is_cloud = self._is_cloud_environment()
        
        if is_cloud:
            # äº‘ç«¯ç¯å¢ƒé…ç½®ï¼ˆLinuxï¼‰
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-software-rasterizer')
            chrome_options.add_argument('--disable-background-timer-throttling')
            chrome_options.add_argument('--disable-backgrounding-occluded-windows')
            chrome_options.add_argument('--disable-renderer-backgrounding')
            chrome_options.add_argument('--disable-features=TranslateUI')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-images')
            chrome_options.add_argument('--virtual-time-budget=5000')
            chrome_options.add_argument('--run-all-compositor-stages-before-draw')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_argument('--disable-features=VizDisplayCompositor')
            self.logger.info("ä½¿ç”¨äº‘ç«¯Linuxç¯å¢ƒé…ç½®")
        else:
            # æœ¬åœ°macOSç¯å¢ƒé…ç½®
            chrome_options.add_argument('--headless=new')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-images')
            chrome_options.add_argument('--disable-features=VizDisplayCompositor')
            chrome_options.add_argument('--disable-ipc-flooding-protection')
            self.logger.info("ä½¿ç”¨æœ¬åœ°macOSç¯å¢ƒé…ç½®")
        
        # é€šç”¨é…ç½®
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--no-default-browser-check')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-popup-blocking')
        chrome_options.add_argument('--disable-translate')
        chrome_options.add_argument('--disable-background-downloads')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-component-extensions-with-background-pages')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--metrics-recording-only')
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--silent')
        chrome_options.add_argument('--log-level=3')
        
        # ç¦ç”¨è‡ªåŠ¨åŒ–æ£€æµ‹
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.logger.info(f"å°è¯•åˆå§‹åŒ–ChromeDriver (ç¬¬{attempt + 1}/{max_retries}æ¬¡)")
                
                # æ¸…ç†æ—§çš„ChromeDriverç¼“å­˜
                if attempt > 0:
                    self.logger.info("æ¸…ç†ChromeDriverç¼“å­˜...")
                    import shutil
                    cache_dir = os.path.expanduser("~/.wdm")
                    if os.path.exists(cache_dir):
                        try:
                            shutil.rmtree(cache_dir)
                        except Exception as e:
                            self.logger.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
                
                # æ€æ­»å¯èƒ½å­˜åœ¨çš„Chromeè¿›ç¨‹
                try:
                    import subprocess
                    subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, check=False)
                    subprocess.run(['pkill', '-f', 'Google Chrome'], capture_output=True, check=False)
                    time.sleep(2)  # ç­‰å¾…è¿›ç¨‹å®Œå…¨é€€å‡º
                except Exception as e:
                    self.logger.debug(f"æ¸…ç†è¿›ç¨‹æ—¶å‡ºé”™: {e}")
                
                # ä½¿ç”¨webdriver-managerè‡ªåŠ¨ç®¡ç†ChromeDriver
                driver_path = ChromeDriverManager().install()
                
                # ä¿®å¤webdriver-managerè·¯å¾„é—®é¢˜
                if 'THIRD_PARTY_NOTICES.chromedriver' in driver_path:
                    driver_dir = os.path.dirname(driver_path)
                    actual_driver_path = os.path.join(driver_dir, 'chromedriver')
                    if os.path.exists(actual_driver_path):
                        driver_path = actual_driver_path
                
                # è®¾ç½®ChromeDriveræƒé™
                self.logger.info(f"è®¾ç½®ChromeDriveræƒé™: {driver_path}")
                os.chmod(driver_path, 0o755)
                
                # ç§»é™¤macOSå®‰å…¨å±æ€§
                try:
                    import subprocess
                    subprocess.run(['xattr', '-d', 'com.apple.quarantine', driver_path], 
                                 capture_output=True, check=False)
                    subprocess.run(['xattr', '-d', 'com.apple.provenance', driver_path], 
                                 capture_output=True, check=False)
                    subprocess.run(['xattr', '-c', driver_path], 
                                 capture_output=True, check=False)
                except Exception as e:
                    self.logger.warning(f"ç§»é™¤å®‰å…¨å±æ€§å¤±è´¥: {e}")
                
                # åˆ›å»ºServiceå¯¹è±¡
                service = Service(driver_path)
                
                # å°è¯•ä¸åŒçš„Chromeé€‰é¡¹ç»„åˆ
                if attempt == 1:
                    # ç¬¬äºŒæ¬¡å°è¯•ï¼šé™çº§åˆ°æ—§çš„æ— å¤´æ¨¡å¼
                    chrome_options.arguments = [arg.replace('--headless=new', '--headless') for arg in chrome_options.arguments]
                    self.logger.info("ç¬¬äºŒæ¬¡å°è¯•ï¼šä½¿ç”¨ä¼ ç»Ÿæ— å¤´æ¨¡å¼")
                elif attempt == 2:
                    # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šæœ€å°åŒ–é€‰é¡¹
                    chrome_options = Options()
                    chrome_options.add_argument('--headless')
                    chrome_options.add_argument('--no-sandbox')
                    chrome_options.add_argument('--disable-dev-shm-usage')
                    chrome_options.add_argument('--disable-gpu')
                    chrome_options.add_argument('--disable-web-security')
                    chrome_options.add_argument('--disable-features=VizDisplayCompositor')
                    self.logger.info("ç¬¬ä¸‰æ¬¡å°è¯•ï¼šä½¿ç”¨æœ€å°åŒ–Chromeé€‰é¡¹")
                
                driver = webdriver.Chrome(service=service, options=chrome_options)
                
                # ä»é…ç½®æ–‡ä»¶è·å–è¶…æ—¶è®¾ç½®
                page_load_timeout = self.config.get_webdriver_page_load_timeout()
                implicit_wait = self.config.get_webdriver_implicit_wait()
                
                driver.set_page_load_timeout(page_load_timeout)
                driver.implicitly_wait(implicit_wait)
                
                self.logger.info(f"WebDriveré…ç½®: é¡µé¢åŠ è½½è¶…æ—¶={page_load_timeout}ç§’, éšå¼ç­‰å¾…={implicit_wait}ç§’")
                self.logger.info("ChromeDriveråˆå§‹åŒ–æˆåŠŸ")
                return driver
                
            except Exception as e:
                self.logger.error(f"Chrome WebDriveråˆå§‹åŒ–å¤±è´¥ (ç¬¬{attempt + 1}/{max_retries}æ¬¡): {e}")
                
                # æ¸…ç†å¯èƒ½æ®‹ç•™çš„è¿›ç¨‹
                try:
                    import subprocess
                    subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, check=False)
                    subprocess.run(['pkill', '-f', 'Google Chrome'], capture_output=True, check=False)
                except:
                    pass
                
                if attempt < max_retries - 1:
                    retry_delay = 5 + (attempt * 2)  # é€’å¢å»¶è¿Ÿ
                    self.logger.info(f"ç­‰å¾…{retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                else:
                    self.logger.error("ChromeDriveråˆå§‹åŒ–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    self.logger.error("å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
                    self.logger.error("1. æ£€æŸ¥Chromeæµè§ˆå™¨æ˜¯å¦å·²å®‰è£…")
                    self.logger.error("2. é‡å¯Terminalå’Œç¨‹åº")
                    self.logger.error("3. åœ¨ç³»ç»Ÿåå¥½è®¾ç½®->å®‰å…¨æ€§ä¸éšç§ä¸­å…è®¸ChromeDriverè¿è¡Œ")
                    raise
    
    def crawl_news(self, target_date: str) -> List[Dict]:
        """çˆ¬å–æŒ‡å®šæ—¥æœŸçš„æ–°é—»æ•°æ®
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
            
        Returns:
            æ–°é—»æ•°æ®åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹çˆ¬å– {target_date} çš„æ–°é—»æ•°æ®")
        
        # é¦–å…ˆå°è¯•Chromeæ¨¡å¼
        try:
            self.logger.info("å°è¯•ä½¿ç”¨Chromeæ¨¡å¼ï¼ˆå®Œæ•´åŠŸèƒ½ï¼‰")
            return self._crawl_with_chrome(target_date)
        except Exception as e:
            self.logger.warning(f"Chromeæ¨¡å¼å¤±è´¥: {e}")
            self.logger.info("åˆ‡æ¢åˆ°requestsæ¨¡å¼ï¼ˆä¿æŒæ—¥æœŸç­›é€‰åŠŸèƒ½ï¼‰")
            return self._crawl_with_requests(target_date)
    
    def _crawl_with_chrome(self, target_date: str) -> List[Dict]:
        """ä½¿ç”¨Chrome WebDriverçˆ¬å–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        driver = None
        try:
            # è®¾ç½®WebDriver
            driver = self._setup_driver()
            
            # è·å–æ–°é—»åˆ—è¡¨é¡µé¢çš„URLåˆ—è¡¨
            news_urls = self._get_news_urls(driver, target_date)
            
            if not news_urls:
                self.logger.warning(f"æœªæ‰¾åˆ° {target_date} çš„æ–°é—»é“¾æ¥")
                return []
            
            self.logger.info(f"æ‰¾åˆ° {len(news_urls)} ä¸ªæ–°é—»é“¾æ¥")
            
            # çˆ¬å–æ¯ç¯‡æ–°é—»çš„è¯¦ç»†å†…å®¹
            news_data = []
            for i, url in enumerate(news_urls, 1):
                self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(news_urls)} ç¯‡æ–°é—»: {url}")
                
                article_data = self._crawl_article(url)
                if article_data:
                    news_data.append(article_data)
                    self.logger.debug(f"æˆåŠŸçˆ¬å–æ–°é—»: {article_data['title'][:50]}...")
                else:
                    self.logger.warning(f"çˆ¬å–æ–°é—»å¤±è´¥: {url}")
                
                # è¯·æ±‚å»¶è¿Ÿ
                time.sleep(self.request_delay)
            
            self.logger.info(f"Chromeæ¨¡å¼çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(news_data)} ç¯‡æ–°é—»")
            return news_data
            
        except Exception as e:
            self.logger.error(f"Chromeæ¨¡å¼çˆ¬å–æ—¶å‡ºé”™: {e}", exc_info=True)
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸»æ–¹æ³•åˆ‡æ¢åˆ°requestsæ¨¡å¼
        finally:
            if driver:
                driver.quit()
    
    def _crawl_with_requests(self, target_date: str) -> List[Dict]:
        """ä½¿ç”¨requestsçˆ¬å–ï¼ˆä¿æŒæ—¥æœŸç­›é€‰é€»è¾‘ï¼‰"""
        try:
            # è·å–æ–°é—»åˆ—è¡¨é¡µé¢çš„URLåˆ—è¡¨
            news_urls = self._get_news_urls_with_requests(target_date)
            
            if not news_urls:
                self.logger.warning(f"æœªæ‰¾åˆ° {target_date} çš„æ–°é—»é“¾æ¥")
                return []
            
            self.logger.info(f"æ‰¾åˆ° {len(news_urls)} ä¸ªæ–°é—»é“¾æ¥")
            
            # çˆ¬å–æ¯ç¯‡æ–°é—»çš„è¯¦ç»†å†…å®¹
            news_data = []
            for i, url in enumerate(news_urls, 1):
                self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(news_urls)} ç¯‡æ–°é—»: {url}")
                
                article_data = self._crawl_article_with_requests(url)
                if article_data:
                    news_data.append(article_data)
                    self.logger.debug(f"æˆåŠŸçˆ¬å–æ–°é—»: {article_data['title'][:50]}...")
                else:
                    self.logger.warning(f"çˆ¬å–æ–°é—»å¤±è´¥: {url}")
                
                # è¯·æ±‚å»¶è¿Ÿ
                time.sleep(self.request_delay)
            
            self.logger.info(f"requestsæ¨¡å¼çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(news_data)} ç¯‡æ–°é—»")
            return news_data
            
        except Exception as e:
            self.logger.error(f"requestsæ¨¡å¼çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def _get_news_urls(self, driver: webdriver.Chrome, target_date: str) -> List[str]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æ–°é—»URLåˆ—è¡¨
        
        Args:
            driver: WebDriverå®ä¾‹
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
            
        Returns:
            æ–°é—»URLåˆ—è¡¨
        """
        try:
            # å°†ç›®æ ‡æ—¥æœŸè½¬æ¢ä¸ºdatetimeå¯¹è±¡
            target_date_obj = datetime.strptime(target_date, '%Y-%m-%d')
            
            all_urls = []
            page = 1  # ä»ç¬¬1é¡µå¼€å§‹
            consecutive_empty_pages = 0
            found_target_news = False  # æ ‡è®°æ˜¯å¦å·²ç»æ‰¾åˆ°è¿‡ç›®æ ‡æ—¥æœŸçš„æ–°é—»
            
            self.logger.info(f"å¼€å§‹çˆ¬å– {target_date} çš„æ–°é—»ï¼Œä½¿ç”¨é€šç”¨ç´¢å¼•é¡µé¢ç­–ç•¥ï¼Œä»ç¬¬{page}é¡µå¼€å§‹")
            
            while True:
                # æ„å»ºç´¢å¼•é¡µé¢URL
                url = f"{self.base_url}/indeks?page={page}"
                
                self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")
                
                # é‡è¯•æœºåˆ¶åŠ è½½é¡µé¢
                page_loaded = False
                max_retries = self.config.get_webdriver_max_retries()
                explicit_wait = self.config.get_webdriver_explicit_wait()
                
                for attempt in range(max_retries):
                    try:
                        self.logger.debug(f"å°è¯•åŠ è½½é¡µé¢ (ç¬¬{attempt+1}/{max_retries}æ¬¡): {url}")
                        driver.get(url)
                        
                        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                        WebDriverWait(driver, explicit_wait).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        
                        # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹åŠ è½½
                        time.sleep(2)
                        page_loaded = True
                        self.logger.debug(f"é¡µé¢åŠ è½½æˆåŠŸ: {url}")
                        break
                        
                    except TimeoutException as e:
                        self.logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ (ç¬¬{attempt+1}/{max_retries}æ¬¡å°è¯•): {url} - {e}")
                        if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                            retry_delay = min(5 * (attempt + 1), 15)  # é€’å¢å»¶è¿Ÿï¼Œæœ€å¤š15ç§’
                            self.logger.info(f"ç­‰å¾…{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                        continue
                    except Exception as e:
                        self.logger.error(f"é¡µé¢åŠ è½½å‡ºé”™ (ç¬¬{attempt+1}/{max_retries}æ¬¡å°è¯•): {url} - {e}")
                        if attempt < max_retries - 1:
                            retry_delay = min(3 * (attempt + 1), 10)
                            time.sleep(retry_delay)
                        continue
                
                if not page_loaded:
                    self.logger.error(f"é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡ç¬¬ {page} é¡µ: {url}")
                    page += 1
                    continue
                
                # è·å–å½“å‰é¡µé¢çš„æ–°é—»é¡¹ç›®ï¼ˆåŒ…å«é“¾æ¥å’Œæ—¶é—´ä¿¡æ¯ï¼‰
                page_urls = self._extract_news_urls_with_time_filter(driver, target_date_obj)
                
                if not page_urls:
                    consecutive_empty_pages += 1
                    self.logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„æ–°é—»")
                    
                    # å¦‚æœå·²ç»æ‰¾åˆ°è¿‡ç›®æ ‡æ—¥æœŸçš„æ–°é—»ï¼Œç°åœ¨åˆæ²¡æœ‰äº†ï¼Œè¯´æ˜å·²ç»è¿‡äº†ç›®æ ‡æ—¥æœŸï¼Œç›´æ¥åœæ­¢
                    if found_target_news:
                        self.logger.info(f"å·²æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–°é—»åå‡ºç°ç©ºé¡µï¼Œè¯´æ˜å·²è¿‡ç›®æ ‡æ—¥æœŸï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    # å¦‚æœä»å¼€å§‹å°±è¿ç»­20é¡µéƒ½æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„æ–°é—»ï¼Œåœæ­¢çˆ¬å–
                    if consecutive_empty_pages >= 20:
                        self.logger.info(f"è¿ç»­{consecutive_empty_pages}é¡µæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„æ–°é—»ï¼Œåœæ­¢çˆ¬å–")
                        break
                else:
                    consecutive_empty_pages = 0
                    found_target_news = True  # æ ‡è®°å·²ç»æ‰¾åˆ°è¿‡ç›®æ ‡æ—¥æœŸçš„æ–°é—»
                    # æ·»åŠ åˆ°æ€»åˆ—è¡¨ï¼Œå»é‡
                    new_urls = [url for url in page_urls if url not in all_urls]
                    all_urls.extend(new_urls)
                    
                    self.logger.info(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(new_urls)} ä¸ªç›®æ ‡æ—¥æœŸçš„æ–°é—»é“¾æ¥")
                
                page += 1
                
                # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šçˆ¬å–50é¡µ
                if page > 50:
                    self.logger.warning("å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶(50é¡µ)")
                    break
            
            self.logger.info(f"æ€»å…±æ‰¾åˆ° {len(all_urls)} ä¸ªç›®æ ‡æ—¥æœŸçš„æ–°é—»é“¾æ¥")
            return all_urls
            
        except Exception as e:
            self.logger.error(f"è·å–æ–°é—»URLåˆ—è¡¨æ—¶å‡ºé”™: {e}", exc_info=True)
            return []
    
    def _parse_time_info(self, time_text: str, title_text: str, target_date: datetime) -> bool:
        """è§£ææ—¶é—´ä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡æ—¥æœŸçš„æ–°é—»
        
        Args:
            time_text: æ˜¾ç¤ºçš„æ—¶é—´æ–‡æœ¬
            title_text: titleå±æ€§ä¸­çš„å®Œæ•´æ—¶é—´ä¿¡æ¯
            target_date: ç›®æ ‡æ—¥æœŸ
            
        Returns:
            bool: æ˜¯å¦ä¸ºç›®æ ‡æ—¥æœŸçš„æ–°é—»
        """
        try:
            # è®¾ç½®é›…åŠ è¾¾æ—¶åŒº
            jakarta_tz = pytz.timezone('Asia/Jakarta')
            now_jakarta = datetime.now(jakarta_tz)
            
            # å°å°¼è¯­æœˆä»½æ˜ å°„ï¼ˆæ”¯æŒæ›´å¤šæ ¼å¼ï¼‰
            month_map = {
                'Jan': 1, 'Januari': 1,
                'Feb': 2, 'Februari': 2,
                'Mar': 3, 'Maret': 3,
                'Apr': 4, 'April': 4,
                'Mei': 5, 'May': 5,
                'Jun': 6, 'Juni': 6,
                'Jul': 7, 'Juli': 7,
                'Agu': 8, 'Agustus': 8,
                'Sep': 9, 'September': 9,
                'Okt': 10, 'Oktober': 10,
                'Nov': 11, 'November': 11,
                'Des': 12, 'Desember': 12
            }
            
            # å°å°¼è¯­æ˜ŸæœŸæ˜ å°„
            day_map = {
                'Minggu': 'Sunday', 'Senin': 'Monday', 'Selasa': 'Tuesday',
                'Rabu': 'Wednesday', 'Kamis': 'Thursday', 'Jumat': 'Friday',
                'Sabtu': 'Saturday'
            }
            
            # æ£€æŸ¥ç»å¯¹æ—¶é—´æ ¼å¼ - åŒæ—¶æ£€æŸ¥time_textå’Œtitle_text
            text_to_check = time_text if time_text else title_text
            if text_to_check and ('WIB' in text_to_check or 'WITA' in text_to_check or 'WIT' in text_to_check):
                self.logger.info(f"ğŸ” è§£æç»å¯¹æ—¶é—´æ ¼å¼: {text_to_check}")
                
                # æ ¼å¼1: Minggu, 03 Agu 2025 13:54 WIB
                pattern1 = r'\w+,\s*(\d{1,2})\s+(\w+)\s+(\d{4})\s+\d{1,2}:\d{2}\s+WI[BTA]'
                match1 = re.search(pattern1, text_to_check)
                if match1:
                    day, month_str, year = match1.groups()
                    if month_str in month_map:
                        try:
                            news_date = datetime(int(year), month_map[month_str], int(day))
                            is_match = news_date.date() == target_date.date()
                            if is_match:
                                self.logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…æ—¥æœŸ(æ ¼å¼1): {text_to_check}")
                            else:
                                self.logger.info(f"âŒ æ—¥æœŸä¸åŒ¹é…(æ ¼å¼1): {news_date.date()} vs {target_date.date()}")
                            return is_match
                        except ValueError as e:
                            self.logger.info(f"âš ï¸ è§£ææ—¥æœŸå¤±è´¥: {e}")
                
                # æ ¼å¼2: 03 Agustus 2025, 13:54 WIB
                pattern2 = r'(\d{1,2})\s+(\w+)\s+(\d{4}),\s*\d{1,2}:\d{2}\s+WI[BTA]'
                match2 = re.search(pattern2, text_to_check)
                if match2:
                    day, month_str, year = match2.groups()
                    if month_str in month_map:
                        try:
                            news_date = datetime(int(year), month_map[month_str], int(day))
                            is_match = news_date.date() == target_date.date()
                            if is_match:
                                self.logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…æ—¥æœŸ(æ ¼å¼2): {text_to_check}")
                            else:
                                self.logger.info(f"âŒ æ—¥æœŸä¸åŒ¹é…(æ ¼å¼2): {news_date.date()} vs {target_date.date()}")
                            return is_match
                        except ValueError as e:
                            self.logger.info(f"âš ï¸ è§£ææ—¥æœŸå¤±è´¥: {e}")
                
                # æ ¼å¼3: 2025-08-03 13:54:00
                pattern3 = r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{2}:\d{2}'
                match3 = re.search(pattern3, text_to_check)
                if match3:
                    year, month, day = match3.groups()
                    try:
                        news_date = datetime(int(year), int(month), int(day))
                        is_match = news_date.date() == target_date.date()
                        if is_match:
                            self.logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…æ—¥æœŸ(æ ¼å¼3): {text_to_check}")
                        else:
                            self.logger.info(f"âŒ æ—¥æœŸä¸åŒ¹é…(æ ¼å¼3): {news_date.date()} vs {target_date.date()}")
                        return is_match
                    except ValueError as e:
                        self.logger.info(f"âš ï¸ è§£ææ—¥æœŸå¤±è´¥: {e}")
                
                self.logger.info(f"âš ï¸ æœªåŒ¹é…ä»»ä½•ç»å¯¹æ—¶é—´æ ¼å¼: {text_to_check}")
            
            # å¤„ç†ç›¸å¯¹æ—¶é—´æ ¼å¼ï¼ˆå¢å¼ºç‰ˆï¼‰
            if 'yang lalu' in time_text or 'lalu' in time_text:
                self.logger.debug(f"å¤„ç†ç›¸å¯¹æ—¶é—´: {time_text}")
                
                # æå–æ•°å­—å’Œæ—¶é—´å•ä½
                time_patterns = [
                    (r'(\d+)\s*menit\s+yang\s+lalu', 'minutes'),
                    (r'(\d+)\s*jam\s+yang\s+lalu', 'hours'),
                    (r'(\d+)\s*hari\s+yang\s+lalu', 'days'),
                    (r'(\d+)\s*minggu\s+yang\s+lalu', 'weeks'),
                    (r'(\d+)\s*bulan\s+yang\s+lalu', 'months'),
                    (r'(\d+)\s*menit\s+lalu', 'minutes'),
                    (r'(\d+)\s*jam\s+lalu', 'hours'),
                    (r'(\d+)\s*hari\s+lalu', 'days')
                ]
                
                for pattern, unit in time_patterns:
                    match = re.search(pattern, time_text, re.IGNORECASE)
                    if match:
                        time_value = int(match.group(1))
                        
                        if unit == 'minutes':
                            news_time = now_jakarta - timedelta(minutes=time_value)
                        elif unit == 'hours':
                            news_time = now_jakarta - timedelta(hours=time_value)
                        elif unit == 'days':
                            news_time = now_jakarta - timedelta(days=time_value)
                        elif unit == 'weeks':
                            news_time = now_jakarta - timedelta(weeks=time_value)
                        elif unit == 'months':
                            # è¿‘ä¼¼è®¡ç®—æœˆä»½ï¼ˆ30å¤©ï¼‰
                            news_time = now_jakarta - timedelta(days=time_value * 30)
                        else:
                            continue
                        
                        self.logger.debug(f"è®¡ç®—æ—¶é—´: {time_value} {unit} å‰ = {news_time}")
                        
                        # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åŒ¹é…
                        is_match = news_time.date() == target_date.date()
                        self.logger.debug(f"æ—¥æœŸåŒ¹é…æ£€æŸ¥: {news_time.date()} vs {target_date.date()} = {is_match}")
                        if is_match:
                            self.logger.info(f"æ‰¾åˆ°åŒ¹é…æ—¥æœŸ(ç›¸å¯¹æ—¶é—´): {time_text} -> {news_time}")
                        return is_match
            
            # å¤„ç†"ä»Šå¤©"ã€"æ˜¨å¤©"ç­‰ç‰¹æ®Šè¯æ±‡
            special_time_map = {
                'hari ini': 0,
                'today': 0,
                'kemarin': 1,
                'yesterday': 1,
                'kemarin dulu': 2,
                'lusa': -1  # æ˜å¤©ï¼ˆé€šå¸¸ä¸ä¼šå‡ºç°åœ¨æ–°é—»ä¸­ï¼‰
            }
            
            time_text_lower = time_text.lower()
            for special_word, days_offset in special_time_map.items():
                if special_word in time_text_lower:
                    news_date = now_jakarta.date() - timedelta(days=days_offset)
                    is_match = news_date == target_date.date()
                    if is_match:
                        self.logger.info(f"æ‰¾åˆ°åŒ¹é…æ—¥æœŸ(ç‰¹æ®Šè¯æ±‡): {time_text} -> {news_date}")
                    return is_match
            
            # å°è¯•è§£æå…¶ä»–å¯èƒ½çš„æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{1,2})/(\d{1,2})/(\d{4})',  # DD/MM/YYYY
                r'(\d{1,2})-(\d{1,2})-(\d{4})',  # DD-MM-YYYY
                r'(\d{4})/(\d{1,2})/(\d{1,2})',  # YYYY/MM/DD
                r'(\d{4})-(\d{1,2})-(\d{1,2})'   # YYYY-MM-DD
            ]
            
            combined_text = f"{time_text} {title_text}"
            for pattern in date_patterns:
                match = re.search(pattern, combined_text)
                if match:
                    try:
                        if pattern.startswith(r'(\d{4})'):
                            # YYYY format
                            year, month, day = match.groups()
                            news_date = datetime(int(year), int(month), int(day))
                        else:
                            # DD format (assume DD/MM/YYYY)
                            day, month, year = match.groups()
                            news_date = datetime(int(year), int(month), int(day))
                        
                        is_match = news_date.date() == target_date.date()
                        if is_match:
                            self.logger.info(f"æ‰¾åˆ°åŒ¹é…æ—¥æœŸ(æ•°å­—æ ¼å¼): {match.group(0)} -> {news_date}")
                        return is_match
                    except ValueError as e:
                        self.logger.debug(f"è§£ææ•°å­—æ—¥æœŸå¤±è´¥: {match.group(0)} - {e}")
                        continue
            
            # å¦‚æœæ‰€æœ‰è§£æéƒ½å¤±è´¥ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
            self.logger.debug(f"æ— æ³•è§£ææ—¶é—´ä¿¡æ¯: time_text='{time_text}', title_text='{title_text}'")
            return False
            
        except Exception as e:
            self.logger.error(f"è§£ææ—¶é—´ä¿¡æ¯æ—¶å‡ºé”™: {time_text}, {title_text} - {e}")
            return False

    def _extract_news_urls_with_time_filter(self, driver: webdriver.Chrome, target_date: datetime) -> List[str]:
        """ä»é¡µé¢ä¸­æå–æ–°é—»URLå¹¶æŒ‰æ—¶é—´ç­›é€‰
        
        Args:
            driver: WebDriverå®ä¾‹
            target_date: ç›®æ ‡æ—¥æœŸ
            
        Returns:
            ç¬¦åˆç›®æ ‡æ—¥æœŸçš„æ–°é—»URLåˆ—è¡¨
        """
        news_urls = []
        
        try:
            # ç­‰å¾…æ–°é—»åˆ—è¡¨åŠ è½½ï¼ˆå‡å°‘è¶…æ—¶æ—¶é—´ï¼‰
            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "article, .media, .list-content__item, .media-artikel"))
                )
            except TimeoutException:
                self.logger.warning("ç­‰å¾…æ–°é—»åˆ—è¡¨åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­å¤„ç†")
                # ä¸ç›´æ¥è¿”å›ï¼Œå°è¯•æŸ¥æ‰¾å·²åŠ è½½çš„å…ƒç´ 
                pass
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»é¡¹ç›®å®¹å™¨
            news_items = driver.find_elements(By.CSS_SELECTOR, "article, .media, .list-content__item, .media-artikel")
            
            for item in news_items:
                try:
                    # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                    link_elem = item.find_element(By.CSS_SELECTOR, "a[href*='/berita/'], a[href*='/news/'], a[href*='detik.com']")
                    href = link_elem.get_attribute('href')
                    
                    if not href or 'detik.com' not in href:
                        continue
                    
                    # æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯
                    time_elem = None
                    time_selectors = [
                        '.media__date', '.date', '.time', 'time', 
                        '[class*="date"]', '[class*="time"]',
                        '.media__subtitle', '.subtitle'
                    ]
                    
                    for selector in time_selectors:
                        try:
                            time_elem = item.find_element(By.CSS_SELECTOR, selector)
                            if time_elem:
                                break
                        except:
                            continue
                    
                    if time_elem:
                         time_text = time_elem.text.strip()
                         # å°è¯•è·å–titleå±æ€§ï¼ˆåŒ…å«å®Œæ•´æ—¶é—´ä¿¡æ¯ï¼‰
                         title_text = time_elem.get_attribute("title") or ""
                         
                         # å¦‚æœtime_elemæ²¡æœ‰titleå±æ€§ï¼Œå°è¯•æŸ¥æ‰¾å­å…ƒç´ çš„titleå±æ€§
                         if not title_text:
                             try:
                                 span_elem = time_elem.find_element(By.TAG_NAME, "span")
                                 title_text = span_elem.get_attribute("title") or ""
                             except:
                                 pass
                         
                         # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ—¥æœŸçš„æ–°é—»
                         if self._parse_time_info(time_text, title_text, target_date):
                             full_url = urljoin(self.base_url, href)
                             if full_url not in news_urls:
                                 news_urls.append(full_url)
                                 self.logger.debug(f"æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–°é—»: {time_text} ({title_text}) - {href}")
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´ä¿¡æ¯ï¼Œä¹Ÿæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼ˆåç»­é€šè¿‡æ–‡ç« é¡µé¢éªŒè¯ï¼‰
                        full_url = urljoin(self.base_url, href)
                        if full_url not in news_urls:
                            news_urls.append(full_url)
                            self.logger.debug(f"æ— æ—¶é—´ä¿¡æ¯çš„æ–°é—»: {href}")
                            
                except Exception as e:
                    self.logger.debug(f"å¤„ç†æ–°é—»é¡¹ç›®æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"æå–åˆ° {len(news_urls)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–°é—»é“¾æ¥")
            return news_urls
            
        except Exception as e:
            self.logger.error(f"æå–æ–°é—»URLæ—¶å‡ºé”™: {e}")
            return []
    
    def _validate_article_data(self, article_data: Dict) -> bool:
        """éªŒè¯æ–‡ç« æ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡
        
        Args:
            article_data: æ–‡ç« æ•°æ®å­—å…¸
            
        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['title', 'publish_time', 'content', 'url']
            for field in required_fields:
                if field not in article_data or not article_data[field]:
                    self.logger.warning(f"æ–‡ç« æ•°æ®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                    return False
            
            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦ï¼ˆè‡³å°‘5ä¸ªå­—ç¬¦ï¼Œæœ€å¤š200ä¸ªå­—ç¬¦ï¼‰
            title = article_data['title'].strip()
            if len(title) < 5 or len(title) > 200:
                self.logger.warning(f"æ ‡é¢˜é•¿åº¦å¼‚å¸¸: {len(title)} å­—ç¬¦ - {title[:50]}...")
                return False
            
            # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼ˆè‡³å°‘50ä¸ªå­—ç¬¦ï¼Œè§†é¢‘æ–°é—»ä¾‹å¤–ï¼‰
            content = article_data['content'].strip()
            # è§†é¢‘æ–°é—»ç‰¹æ®Šå¤„ç†
            if '[VIDEOæ–°é—»]' in content:
                if len(content) < 20:  # è§†é¢‘æ–°é—»æœ€å°‘20å­—ç¬¦
                    self.logger.warning(f"è§†é¢‘æ–°é—»å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦ - {content[:50]}...")
                    return False
            else:
                if len(content) < 50:  # æ™®é€šæ–°é—»è‡³å°‘50å­—ç¬¦
                    self.logger.warning(f"å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦ - {content[:50]}...")
                    return False
            
            # æ£€æŸ¥URLæ ¼å¼
            url = article_data['url']
            if not url.startswith('https://') or 'detik.com' not in url:
                self.logger.warning(f"URLæ ¼å¼å¼‚å¸¸: {url}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„é”™è¯¯å†…å®¹
            error_indicators = [
                '404', 'not found', 'error', 'halaman tidak ditemukan',
                'access denied', 'forbidden', 'server error'
            ]
            
            content_lower = content.lower()
            title_lower = title.lower()
            
            for indicator in error_indicators:
                if indicator in content_lower or indicator in title_lower:
                    self.logger.warning(f"æ£€æµ‹åˆ°é”™è¯¯å†…å®¹æŒ‡ç¤ºå™¨: {indicator}")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"éªŒè¯æ–‡ç« æ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def _crawl_article(self, url: str) -> Optional[Dict]:
        """çˆ¬å–å•ç¯‡æ–°é—»æ–‡ç« 
        
        Args:
            url: æ–°é—»æ–‡ç« URL
            
        Returns:
            æ–°é—»æ•°æ®å­—å…¸ï¼ŒåŒ…å«titleã€publish_timeã€content
        """
        # åªå¤„ç† https://news.detik.com/berita å¼€å¤´çš„é“¾æ¥
        if not url.startswith('https://news.detik.com/berita'):
            self.logger.info(f"è·³è¿‡ä¸ç¬¦åˆæ¡ä»¶çš„é“¾æ¥: {url}")
            return None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºvideoæ–°é—»ï¼ˆ20.detik.comå¼€å¤´ï¼‰
        if url.startswith('https://20.detik.com'):
            self.logger.info(f"è¯†åˆ«åˆ°videoæ–°é—»: {url}")
            try:
                response = self.session.get(url, timeout=self.request_timeout)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title = self._extract_title(soup)
                if not title:
                    title = "Videoæ–°é—»ï¼ˆæ— æ³•æå–æ ‡é¢˜ï¼‰"
                
                # æå–å‘å¸ƒæ—¶é—´
                publish_time = self._extract_publish_time(soup)
                
                article_data = {
                    'title': title.strip(),
                    'publish_time': publish_time.strip() if publish_time else '',
                    'content': '[VIDEOæ–°é—»]',
                    'url': url
                }
                
                # éªŒè¯æ•°æ®è´¨é‡
                if self._validate_article_data(article_data):
                    return article_data
                else:
                    self.logger.warning(f"Videoæ–°é—»æ•°æ®éªŒè¯å¤±è´¥: {url}")
                    return None
            except Exception as e:
                self.logger.warning(f"çˆ¬å–videoæ–°é—»å¤±è´¥: {url}, é”™è¯¯: {e}")
                return {
                    'title': 'Videoæ–°é—»ï¼ˆæ— æ³•æå–æ ‡é¢˜ï¼‰',
                    'publish_time': '',
                    'content': '[VIDEOæ–°é—»]',
                    'url': url
                }
        
        # å¤„ç†æ™®é€šæ–°é—»
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.request_timeout)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title = self._extract_title(soup)
                if not title:
                    self.logger.warning(f"æ— æ³•æå–æ ‡é¢˜: {url}")
                    continue
                
                # æå–å‘å¸ƒæ—¶é—´
                publish_time = self._extract_publish_time(soup)
                
                # æå–æ­£æ–‡å†…å®¹
                content = self._extract_content(soup)
                if not content:
                    self.logger.warning(f"æ— æ³•æå–å†…å®¹: {url}")
                    continue
                
                article_data = {
                    'title': title.strip(),
                    'publish_time': publish_time.strip() if publish_time else '',
                    'content': content.strip(),
                    'url': url
                }
                
                # éªŒè¯æ•°æ®è´¨é‡
                if self._validate_article_data(article_data):
                    return article_data
                else:
                    self.logger.warning(f"æ–‡ç« æ•°æ®éªŒè¯å¤±è´¥: {url}")
                    continue
                
            except Exception as e:
                self.logger.warning(f"çˆ¬å–æ–‡ç« å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {url}, é”™è¯¯: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ–°é—»æ ‡é¢˜
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æ–°é—»æ ‡é¢˜
        """
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [
            'h1.detail__title',
            'h1[class*="title"]',
            '.detail__title',
            'h1',
            '.entry-title',
            '[class*="headline"]'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                return title_elem.get_text(strip=True)
        
        return None
    
    def _extract_publish_time(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–å‘å¸ƒæ—¶é—´
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            å‘å¸ƒæ—¶é—´
        """
        # å°è¯•å¤šç§æ—¶é—´é€‰æ‹©å™¨
        time_selectors = [
            '.detail__date',
            '[class*="date"]',
            '[class*="time"]',
            'time',
            '.published-date',
            '.entry-date'
        ]
        
        for selector in time_selectors:
            time_elem = soup.select_one(selector)
            if time_elem:
                # å°è¯•è·å–datetimeå±æ€§
                datetime_attr = time_elem.get('datetime')
                if datetime_attr:
                    return datetime_attr
                return time_elem.get_text(strip=True)
        
        return None
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤HTMLå®ä½“
        import html
        text = html.unescape(text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        # ç§»é™¤å¸¸è§çš„å¹¿å‘Šå’Œæ— å…³æ–‡æœ¬
        unwanted_patterns = [
            r'Baca juga:.*?(?=\n|$)',
            r'ADVERTISEMENT.*?(?=\n|$)',
            r'Simak Video.*?(?=\n|$)',
            r'\(\w+/\w+\)',  # ç§»é™¤ç±»ä¼¼ (detik/detik) çš„æ ‡è®°
            r'Halaman selanjutnya.*?(?=\n|$)',
            r'Lanjutkan membaca.*?(?=\n|$)'
        ]
        
        for pattern in unwanted_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„æ¢è¡Œç¬¦
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ–°é—»æ­£æ–‡å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æ–°é—»æ­£æ–‡å†…å®¹
        """
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        content_selectors = [
            '.detail__body-text',
            '.itp_bodycontent',
            '.detail-content',
            '[class*="content"]',
            '[class*="body"]',
            '.entry-content',
            '.article-content',
            '.post-content',
            '.content',
            'article'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                unwanted_selectors = [
                    'script', 'style', 'noscript',
                    '.ads', '.advertisement', '.related', '.share',
                    '.social', '.comment', '.navigation',
                    '[class*="ad"]', '[id*="ad"]',
                    '.widget', '.sidebar'
                ]
                
                for unwanted_selector in unwanted_selectors:
                    for unwanted in content_elem.select(unwanted_selector):
                        unwanted.decompose()
                
                # è·å–æ‰€æœ‰æ®µè½æ–‡æœ¬
                paragraphs = content_elem.find_all(['p', 'div', 'span'])
                if paragraphs:
                    content_parts = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬å’Œæ— æ„ä¹‰çš„æ–‡æœ¬
                        if text and len(text) > 15 and not text.isdigit():
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®é™…å†…å®¹ï¼ˆä¸åªæ˜¯æ ‡ç‚¹ç¬¦å·ï¼‰
                            if re.search(r'[a-zA-Z\u00C0-\u017F\u0100-\u024F]', text):
                                cleaned_text = self._clean_text(text)
                                if cleaned_text and len(cleaned_text) > 10:
                                    content_parts.append(cleaned_text)
                    
                    if content_parts:
                        full_content = '\n\n'.join(content_parts)
                        return self._clean_text(full_content)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½ï¼Œç›´æ¥è·å–æ–‡æœ¬
                raw_text = content_elem.get_text(strip=True)
                if raw_text:
                    return self._clean_text(raw_text)
        
        return None
    
    # ===== ä½¿ç”¨requestsçš„æ–¹æ³•ï¼ˆChromeå¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰=====
    
    def _get_news_urls_with_requests(self, target_date: str) -> List[str]:
        """ä½¿ç”¨requestsè·å–æŒ‡å®šæ—¥æœŸçš„æ–°é—»URLåˆ—è¡¨ï¼ˆä¿æŒåŸæœ‰çš„æ—¥æœŸç­›é€‰é€»è¾‘ï¼‰"""
        try:
            target_date_obj = datetime.strptime(target_date, '%Y-%m-%d')
            all_urls = []
            page = 1
            consecutive_empty_pages = 0
            found_target_news = False
            
            self.logger.info(f"å¼€å§‹ä½¿ç”¨requestsçˆ¬å– {target_date} çš„æ–°é—»ï¼Œä»ç¬¬{page}é¡µå¼€å§‹")
            
            while True:
                url = f"{self.base_url}/indeks?page={page}"
                self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")
                
                try:
                    response = self.session.get(url, timeout=self.request_timeout)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    page_urls = self._extract_news_urls_with_requests(soup, target_date_obj)
                    
                    if not page_urls:
                        consecutive_empty_pages += 1
                        self.logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„æ–°é—»")
                        
                        # å¦‚æœå·²ç»æ‰¾åˆ°è¿‡ç›®æ ‡æ—¥æœŸçš„æ–°é—»ï¼Œç°åœ¨åˆæ²¡æœ‰äº†ï¼Œè¯´æ˜å·²ç»è¿‡äº†ç›®æ ‡æ—¥æœŸï¼Œç›´æ¥åœæ­¢
                        if found_target_news:
                            self.logger.info(f"å·²æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–°é—»åå‡ºç°ç©ºé¡µï¼Œè¯´æ˜å·²è¿‡ç›®æ ‡æ—¥æœŸï¼Œåœæ­¢çˆ¬å–")
                            break
                        
                        # å¦‚æœä»å¼€å§‹å°±è¿ç»­20é¡µéƒ½æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„æ–°é—»ï¼Œåœæ­¢çˆ¬å–
                        if consecutive_empty_pages >= 20:
                            self.logger.info(f"è¿ç»­{consecutive_empty_pages}é¡µæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„æ–°é—»ï¼Œåœæ­¢çˆ¬å–")
                            break
                    else:
                        consecutive_empty_pages = 0
                        found_target_news = True
                        # æ·»åŠ åˆ°æ€»åˆ—è¡¨ï¼Œå»é‡
                        new_urls = [url for url in page_urls if url not in all_urls]
                        all_urls.extend(new_urls)
                        
                        self.logger.info(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(new_urls)} ä¸ªç›®æ ‡æ—¥æœŸçš„æ–°é—»é“¾æ¥")
                    
                    page += 1
                    
                    # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šçˆ¬å–50é¡µ
                    if page > 50:
                        self.logger.info("å·²è¾¾åˆ°æœ€å¤§é¡µé¢æ•°é™åˆ¶ï¼ˆ50é¡µï¼‰ï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    time.sleep(1)  # é¡µé¢é—´å»¶è¿Ÿ
                    
                except Exception as e:
                    self.logger.error(f"çˆ¬å–ç¬¬ {page} é¡µå¤±è´¥: {e}")
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= 5:
                        break
                    continue
            
            self.logger.info(f"requestsæ¨¡å¼å…±æ‰¾åˆ° {len(all_urls)} ä¸ªæ–°é—»é“¾æ¥")
            return all_urls
            
        except Exception as e:
            self.logger.error(f"ä½¿ç”¨requestsè·å–æ–°é—»URLåˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return []
    
    def _extract_news_urls_with_requests(self, soup: BeautifulSoup, target_date: datetime) -> List[str]:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ–°é—»URLå¹¶æŒ‰æ—¶é—´ç­›é€‰"""
        news_urls = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»é¡¹ç›®å®¹å™¨
            news_items = soup.select("article, .media, .list-content__item, .media-artikel")
            
            for item in news_items:
                try:
                    # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                    link_element = item.select_one("a[href*='/berita/']")
                    if not link_element:
                        continue
                    
                    href = link_element.get('href')
                    if not href:
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = urljoin(self.base_url, href)
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue
                    
                    # åªå¤„ç† news.detik.com/berita å¼€å¤´çš„é“¾æ¥
                    if not full_url.startswith('https://news.detik.com/berita'):
                        continue
                    
                    # æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯ - æ‰©å±•é€‰æ‹©å™¨
                    time_element = item.select_one(".media__date, .list-content__date, [class*='date'], [class*='time'], time, .date, .time, .timestamp")
                    title_element = item.select_one(".media__title, .list-content__title, h2, h3, h4, a")
                    
                    time_text = time_element.get_text(strip=True) if time_element else ""
                    title_text = title_element.get_text(strip=True) if title_element else ""
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´å…ƒç´ ï¼Œå°è¯•ä»æ•´ä¸ªé¡¹ç›®ä¸­æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯
                    if not time_text:
                        # æŸ¥æ‰¾åŒ…å«æ—¶é—´æ ¼å¼çš„æ‰€æœ‰æ–‡æœ¬
                        all_text = item.get_text()
                        import re
                        # æŸ¥æ‰¾WIBæ ¼å¼çš„æ—¶é—´
                        wib_match = re.search(r'[^.]*\d{1,2}\s+\w+\s+\d{4}\s+\d{1,2}:\d{2}\s+WIB[^.]*', all_text)
                        if wib_match:
                            time_text = wib_match.group(0).strip()
                    
                    # è°ƒè¯•æ—¥å¿— - è®°å½•æå–åˆ°çš„ä¿¡æ¯ï¼ˆæ”¹ä¸ºINFOçº§åˆ«ä¾¿äºè°ƒè¯•ï¼‰
                    if time_text or title_text:
                        self.logger.info(f"ğŸ” æ£€æŸ¥æ–°é—»é¡¹ç›® - æ—¶é—´: '{time_text}', æ ‡é¢˜: '{title_text[:30]}...'")
                    
                    # ä½¿ç”¨åŸæœ‰çš„æ—¶é—´è§£æé€»è¾‘
                    if self._parse_time_info(time_text, title_text, target_date):
                        news_urls.append(full_url)
                        self.logger.info(f"âœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–°é—»: {title_text[:50]}...")
                    elif time_text:
                        self.logger.info(f"âŒ æ—¶é—´ä¸åŒ¹é…: '{time_text}' vs ç›®æ ‡æ—¥æœŸ: {target_date.strftime('%Y-%m-%d')}")
                    
                except Exception as e:
                    self.logger.debug(f"å¤„ç†æ–°é—»é¡¹ç›®æ—¶å‡ºé”™: {e}")
                    continue
            
            return news_urls
            
        except Exception as e:
            self.logger.error(f"ä»é¡µé¢æå–æ–°é—»URLæ—¶å‡ºé”™: {e}")
            return []
    
    def _crawl_article_with_requests(self, url: str) -> Optional[Dict]:
        """ä½¿ç”¨requestsçˆ¬å–å•ç¯‡æ–°é—»æ–‡ç« """
        # åªå¤„ç† https://news.detik.com/berita å¼€å¤´çš„é“¾æ¥
        if not url.startswith('https://news.detik.com/berita'):
            self.logger.info(f"è·³è¿‡ä¸ç¬¦åˆæ¡ä»¶çš„é“¾æ¥: {url}")
            return None
        
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.request_timeout)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title = self._extract_title_with_requests(soup)
                if not title:
                    continue
                
                # æå–å‘å¸ƒæ—¶é—´
                publish_time = self._extract_publish_time_with_requests(soup)
                
                # æå–æ­£æ–‡å†…å®¹
                content = self._extract_content_with_requests(soup)
                if not content:
                    continue
                
                # ç”Ÿæˆæ–‡ç« ID
                article_id = len(url.split('/'))  # ç®€å•çš„IDç”Ÿæˆ
                
                return {
                    'id': article_id,
                    'title': title.strip(),
                    'publish_time': publish_time.strip() if publish_time else '',
                    'content': content.strip(),
                    'url': url,
                    'word_count': len(content.split())
                }
                
            except Exception as e:
                self.logger.warning(f"çˆ¬å–æ–‡ç« å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {url}, é”™è¯¯: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def _extract_title_with_requests(self, soup: BeautifulSoup) -> Optional[str]:
        """ä½¿ç”¨BeautifulSoupæå–æ–°é—»æ ‡é¢˜"""
        selectors = [
            'h1.detail__title',
            'h1[class*="title"]',
            '.detail__title',
            'h1',
            '.entry-title'
        ]
        
        for selector in selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                return title_elem.get_text(strip=True)
        
        return None
    
    def _extract_publish_time_with_requests(self, soup: BeautifulSoup) -> Optional[str]:
        """ä½¿ç”¨BeautifulSoupæå–å‘å¸ƒæ—¶é—´"""
        selectors = [
            '.detail__date',
            '[class*="date"]',
            '[class*="time"]',
            'time',
            '.published-date'
        ]
        
        for selector in selectors:
            time_elem = soup.select_one(selector)
            if time_elem:
                datetime_attr = time_elem.get('datetime')
                if datetime_attr:
                    return datetime_attr
                return time_elem.get_text(strip=True)
        
        return None
    
    def _extract_content_with_requests(self, soup: BeautifulSoup) -> Optional[str]:
        """ä½¿ç”¨BeautifulSoupæå–æ–°é—»æ­£æ–‡å†…å®¹"""
        selectors = [
            '.detail__body-text',
            '.itp_bodycontent',
            '.detail-content',
            '[class*="content"]',
            '.entry-content'
        ]
        
        for selector in selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in content_elem.select('script, style, .ads, .advertisement'):
                    unwanted.decompose()
                
                # è·å–æ–‡æœ¬å†…å®¹
                text = content_elem.get_text(separator='\n', strip=True)
                if text and len(text) > 50:
                    return self._clean_text_requests(text)
        
        return None
    
    def _clean_text_requests(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹ï¼ˆrequestsç‰ˆæœ¬ï¼‰"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        import re
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
